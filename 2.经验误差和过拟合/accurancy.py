import pandas as pd#
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import  (accuracy_score,precision_score,f1_score,recall_score,roc_auc_score,roc_curve,precision_recall_curve,average_precision_score,log_loss,confusion_matrix)
data = pd.read_csv('Synthetic_Watermelon_Dataset__10000_rows_.csv', header=0, encoding='utf-8')#é€šè¿‡pd.read_csvæ–¹å¼è¯»å–æ•°æ®ï¼ŒUTF-8ï¼Œheader=0æŒ‡çš„æ˜¯ç¬¬ä¸€è¡Œå°±æ˜¯åˆ—æ˜
data = data.rename(columns={'å¥½ç“œ(1=å¥½;0=å)': 'å¥½ç“œ'})#é‡å‘½åæœ€åä¸€åˆ—åå­—
#data = data[:8]
x = data[['å¯†åº¦', 'å«ç³–ç‡']].values#ä»dataframeä¸­é€‰å–å¯†åº¦ã€å«ç³–ç‡ä¸¤åˆ—ä½œä¸ºè¾“å…¥ç‰¹å¾
y = data["å¥½ç“œ"].values#æœ€åä¸€åˆ—ä½œä¸ºæ ‡ç­¾

print(y)
np.random.seed(0)#è®¾ç½®ç§å­
indices = np.random.permutation(len(x))#ç”Ÿæˆä¸€ä¸ª0-19çš„éšæœºæ’åˆ—-æ„ä¹‰åœ¨ä¸æ‰“ä¹±é¡ºåºï¼Œå°†å†…éƒ¨åˆ†å¸ƒå‡åŒ€åŒ–
train_idx = indices[:8000]#å–å‰16ä¸ªç´¢å¼•ä¸ºè®­ç»ƒé›†åˆ
test_idx = indices[8000:]#å…¶ä½™çš„ä¸ºæµ‹è¯•é›†åˆ
x_train,y_train = x[train_idx],y[train_idx]#æŒ‰ç…§é€‰å®šçš„ç´¢å¼•åˆ‡ç‰‡ï¼Œå¾—åˆ°è®­ç»ƒé›†ç‰¹å¾ X_trainï¼ˆå½¢çŠ¶ (16,2)ï¼‰å’Œè®­ç»ƒæ ‡ç­¾ y_trainï¼ˆé•¿åº¦ 16ï¼‰
x_test,y_test = x[test_idx],y[test_idx]#æŒ‰ç…§é€‰å®šçš„ç´¢å¼•åˆ‡ç‰‡ï¼Œå¾—åˆ°è®­ç»ƒé›†ç‰¹å¾ X_trainï¼ˆå½¢çŠ¶ (4,2)ï¼‰å’Œè®­ç»ƒæ ‡ç­¾ y_trainï¼ˆé•¿åº¦ 4ï¼‰ã€‚
model = LogisticRegression( )#å®ä¾‹åŒ–ä¸€ä¸ª scikit-learn çš„é€»è¾‘å›å½’æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆL2 æ­£åˆ™åŒ–ã€lbfgs ä¼˜åŒ–å™¨ç­‰ï¼‰ã€‚scikit-learnï¼ˆä¹Ÿå¸¸å†™ä½œ
#â€œsklearnâ€ï¼‰æ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„ã€ç”¨çº¯ Python å†™æˆçš„æœºå™¨å­¦ä¹ åº“ï¼Œå…¨ç§°æ˜¯ â€œScikit-Learnâ€ã€‚å®ƒä»¥ç®€å•æ˜“ç”¨ã€æ¥å£ç»Ÿä¸€ã€æ–‡æ¡£å®Œå–„è‘—ç§°ï¼Œå‡ ä¹æ¶µç›–äº†å¸¸è§çš„ç›‘ç£å­¦ä¹ å’Œéç›‘ç£å­¦ä¹ ç®—


#æ³•ï¼šçº¿æ€§/é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€èšç±»ã€é™ç»´ã€æ¨¡å‹é€‰æ‹©ã€è¯„ä¼°æŒ‡æ ‡â€¦â€¦ç­‰ç­‰;
# LogisticRegression(
  #  penalty='l2',          # ä½¿ç”¨ L2 æ­£åˆ™åŒ–ï¼š'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'å¤§å¤šæ•°ä¼˜åŒ–å™¨éƒ½æ”¯æŒ L2ï¼›l1'	'liblinear', 'saga'	åªæœ‰è¿™ä¸¤ç§ solver æ”¯æŒ L1
  
  #  dual=False,            # ä¸ä½¿ç”¨å¯¹å¶ï¼ˆdualï¼‰å®ç°ï¼Œé€‚åˆ n_samples > n_featuresï¼ˆæ ·æœ¬æ•°>ç‰¹å¾ç»´æ•°ï¼‰å½“ n_samples > n_featuresï¼ˆæ ·æœ¬æ¯”ç‰¹å¾å¤šï¼‰æ—¶ï¼ŒåŸå§‹ï¼ˆ
  # primalï¼‰æ–¹æ³•é€šå¸¸æ›´å¿«ï¼Œå°± dual=Falseã€‚å½“ n_features > n_samplesï¼ˆç‰¹å¾æ¯”æ ·æœ¬å¤šï¼‰æ—¶ï¼Œå¯¹å¶ï¼ˆdualï¼‰æ–¹æ³•å¯èƒ½æ›´é«˜æ•ˆï¼Œå°±å¯ä»¥è€ƒè™‘ dual=Trueï¼ˆå‰ææ˜¯ä½ ç”¨çš„ solver 
  # æ”¯æŒå¯¹å¶ï¼Œä¹Ÿå°±æ˜¯ solver='liblinear' å¹¶ä¸” penalty='l2'ï¼‰ã€‚

   # tol=1e-4,              # æ”¶æ•›é˜ˆå€¼åœ¨æ¯ä¸€æ¬¡è¿­ä»£ç»“æŸåï¼Œç®—æ³•ä¼šè®¡ç®—å½“å‰è¿™ä¸€æ­¥çš„â€œç›®æ ‡å‡½æ•°å€¼â€ï¼ˆä¹Ÿå°±æ˜¯å¸¦æ­£åˆ™åŒ–çš„å¯¹æ•°ä¼¼ç„¶ï¼‰æˆ–ç›¸é‚»ä¸¤æ¬¡å‚æ•°å˜åŒ–çš„å¤§å°ï¼Œæ£€æŸ¥å®ƒä»¬ä¸ä¸Šæ¬¡ç›¸æ¯”
   # å˜åŒ–æœ‰å¤šå¤§ã€‚å½“â€œå˜åŒ–é‡â€å°åˆ°ä½äºæˆ‘ä»¬è®¾å®šçš„å®¹å¿åº¦ï¼ˆtolï¼‰æ—¶ï¼Œå°±è®¤ä¸ºâ€œè¿›ä¸€æ­¥è¿­ä»£å·²ç»ä¸ä¼šå¸¦æ¥æ˜æ˜¾æ”¹å–„â€ï¼Œäºæ˜¯åœæ­¢è¿­ä»£ï¼Œè¾“å‡ºå½“å‰å‚æ•°ä½œä¸ºæœ€ç»ˆè§£ã€‚

   # C=1.0,                 # æ­£åˆ™åŒ–å¼ºåº¦çš„å€’æ•°ï¼ŒC è¶Šå¤§æ­£åˆ™åŒ–è¶Šå¼±

   # fit_intercept=True,    # æ˜¯å¦æ‹Ÿåˆæˆªè·é¡¹,trueæ—¶è¿™æ—¶æ¨¡å‹ä¼šåœ¨è®­ç»ƒæ—¶ä¸€èµ·å­¦ä¹  ğ‘¤ å’Œ ğ‘ã€‚fit_intercept=True æ˜¯é»˜è®¤å€¼ï¼Œä¸€èˆ¬ä¸ç”¨æ˜¾å¼æŒ‡å®šã€‚Falseæ—¶åªå­¦ä¹ W

   # solver='lbfgs',        # ä¼˜åŒ–ç®—æ³•ï¼Œè¿™é‡Œé»˜è®¤ç”¨ L-BFGSï¼Œä¼˜åŒ–å™¨ï¼ŒL1ï¼ŒL2é€‰æ‹©æ—¶è®°å¾—æ³¨æ„ï¼›
#| Solver        | æ”¯æŒæ­£åˆ™åŒ–                 | æ”¯æŒå¤šåˆ†ç±»æ¨¡å¼          | é€‚åˆæ•°æ®ç±»å‹ | é€‚åˆè§„æ¨¡                    | é€šå¸¸åœºæ™¯                        |
#| ------------- | ------------------------ | ------------------    | -----      | ------------------        | ------------------------       |
#| **liblinear** | L1, L2                   | OvRï¼ˆäºŒåˆ†ç±» & å¤šåˆ†ç±»ï¼‰   | ç¨ å¯† / ç¨€ç– | ä¸­å°è§„æ¨¡ï¼ˆå‡ ä¸‡æ ·æœ¬ã€å‡ åƒç‰¹å¾ï¼‰ | éœ€è¦ L1/L2ã€æ ·æœ¬ & ç‰¹å¾éƒ½ä¸­ç­‰æ—¶    |
#| **lbfgs**     | L2, none                 | æ”¯æŒ Multinomial å¤šåˆ†ç±» | ç¨ å¯†       | å°åˆ°ä¸­è§„æ¨¡ï¼ˆå‡ åƒæ ·æœ¬åˆ°å‡ ä¸‡ç‰¹å¾ï¼‰| åªéœ€ L2ï¼Œå¤šåˆ†ç±»ç²¾åº¦è¾ƒå¥½           |
#| **newton-cg** | L2, none                 | æ”¯æŒ Multinomial å¤šåˆ†ç±» | ç¨ å¯†       | å°åˆ°ä¸­è§„æ¨¡                  | åªéœ€ L2ï¼Œå¤šåˆ†ç±»ï¼ŒäºŒé˜¶æ”¶æ•›å¿«        |
#| **sag**       | L2, none                 | æ”¯æŒ Multinomial å¤šåˆ†ç±» | ç¨ å¯†       | å¤§è§„æ¨¡ï¼ˆ$\ge 10^5$ æ ·æœ¬ï¼‰    | åªéœ€ L2ï¼Œæµ·é‡ç¨ å¯†æ•°æ®             |
#| **saga**      | L1, L2, ElasticNet, none | æ”¯æŒ Multinomial å¤šåˆ†ç±» | ç¨ å¯† / ç¨€ç– | å¤§è§„æ¨¡ï¼ˆ$\ge 10^5$ æ ·æœ¬ï¼‰    | éœ€è¦ L1/ElasticNetï¼Œæµ·é‡æˆ–ç¨€ç–æ•°æ®|
#å¤§é‡æ—¶ä¼˜å…ˆsagæˆ–saga

   # max_iter=100,          # æœ€å¤§è¿­ä»£æ¬¡æ•° max_iter=100 åªæ˜¯å‘Šè¯‰ä¼˜åŒ–å™¨â€œå¦‚æœè¿›è¡Œäº† 100 æ¬¡å‚æ•°æ›´æ–°è¿˜æ²¡æ»¡è¶³æ”¶æ•›æ¡ä»¶ï¼Œå°±åœæ­¢å¹¶è­¦å‘Šæˆ‘â€ã€‚é»˜è®¤å€¼100

   # multi_class='auto',    # å¤šåˆ†ç±»æ¨¡å¼ï¼Œauto ä¼šæ ¹æ®æ ‡ç­¾è‡ªåŠ¨é€‰æ‹© 'ovr'=ã€‹2åˆ†ç±»liblinearä¸æ”¯æŒ æˆ– 'multinomial' ï¼šå¤šåˆ†ç±»
   #| ç‰¹æ€§   | OvR                                                      | Multinomial                            |
   #| ---- | -------------------------------------------------------- | -------------------------------------- |
   #| è®­ç»ƒæ–¹å¼ | å¯¹æ¯ä¸ªç±»åˆ«å•ç‹¬è®­ç»ƒä¸€ä¸ªäºŒåˆ†ç±»æ¨¡å‹ï¼Œå…± $K$ ä¸ªæ¨¡å‹                               | ä¸€æ¬¡è”åˆè®­ç»ƒä¸€ä¸ª $K$-åˆ†ç±»æ¨¡å‹                      |
   #| è®¡ç®—é‡  | $K$ æ¬¡äºŒåˆ†ç±»ï¼ˆæ¯æ¬¡ä¼˜åŒ–ç»´åº¦ä¸º R^dï¼‰                         | ä¸€æ¬¡å¤šé¡¹å¼ Softmax ä¼˜åŒ–ï¼ˆå‚æ•°çŸ©é˜µå¤§å°ä¸º dxkï¼‰ |
   #| é¢„æµ‹é˜¶æ®µ | å…ˆå¹¶è¡Œï¼ˆæˆ–é€ä¸€ï¼‰è®¡ç®—æ¯ä¸ªäºŒåˆ†ç±»å™¨çš„æ¦‚ç‡ï¼Œå†é€‰æœ€å¤§è€…                                | ç›´æ¥ä¸€æ¬¡æ€§è¾“å‡ºæ‰€æœ‰ç±»åˆ«çš„ Softmax æ¦‚ç‡                |
   #| ä¼˜ç‚¹   | - å®ç°ç®€å•<br>- å¯¹å°æ ·æœ¬ã€ç‰¹å¾æé«˜ç»´æ—¶æ›´ç¨³å®š                               | - è”åˆä¼˜åŒ–ï¼Œèƒ½æ›´å¥½åœ°åè°ƒå„ç±»åˆ«é—´ç«äº‰<br>- å¯¹å¤šåˆ†ç±»ç²¾åº¦æ›´å¥½      |
   #| ç¼ºç‚¹   | - å„ä¸ªäºŒåˆ†ç±»å™¨ä¸å…±äº«ä¿¡æ¯ã€ç›¸äº’ç‹¬ç«‹<br>- ç±»åˆ«å¤±è¡¡æ—¶å¯èƒ½è¡¨ç°ä¸ç¨³å®š                     | - å¯¹ç‰¹å¾ç»´åº¦å’Œç±»åˆ«æ•°éƒ½è¾ƒæ•æ„Ÿï¼Œè®¡ç®—å’Œå†…å­˜å¼€é”€æ›´å¤§              |
   #| é€‚ç”¨åœºæ™¯ | - éœ€è¦ L1 æ­£åˆ™åŒ–ï¼ˆå› ä¸º `liblinear` åªæ”¯æŒ OvRï¼‰<br>- æ ·æœ¬é‡å°ã€ç‰¹å¾æç¨€ç–ï¼ˆå¦‚æ–‡æœ¬ï¼‰ | - æ ·æœ¬é‡ä¸­ç­‰æˆ–å¤§ï¼Œç‰¹å¾ç»´åº¦ä¸æç«¯æ—¶<br>- éœ€è¦æœ€ä¼˜çš„å¤šåˆ†ç±»æ€§èƒ½æ—¶    |

   # verbose=0,             # æ˜¯å¦è¾“å‡ºè®­ç»ƒè¿‡ç¨‹æ—¥å¿— 0å¦ 1æ˜¯
   
   # warm_start=False,      # æ˜¯å¦ä½¿ç”¨ä¸Šä¸€æ¬¡è®­ç»ƒçš„æƒé‡ä½œä¸ºåˆå§‹ç‚¹è¿™ä¼šå¯¼è‡´ä¸¤ä¸ªä¸åŒçš„ fit è¿‡ç¨‹äº§ç”Ÿâ€œä¾èµ–å…³ç³»â€â€”â€”ç¬¬äºŒæ¬¡çš„ç»“æœä¼šæ·±å—ç¬¬ä¸€æ¬¡è®­ç»ƒç»“æœçš„å½±å“ï¼Œ
   # ç ´åäº†â€œæ¯æ¬¡è°ƒç”¨ fit éƒ½æ˜¯ä»åŒä¸€èµ·ç‚¹å¼€å§‹â€çš„ä¸€è‡´æ€§ï¼Œé™ä½äº†ç»“æœçš„å¯å¤ç°æ€§ã€‚â€œå¢é‡å¼è®­ç»ƒï¼ˆwarm startï¼‰â€å¸¸è§äºéå¸¸å¤§çš„æ•°æ®æµã€åœ¨çº¿å­¦ä¹ æˆ–éœ€è¦åˆ†æ‰¹æ¬¡å¤šæ¬¡å¾®è°ƒåŒä¸€æ¨¡å‹çš„åœºæ™¯ã€‚

   # n_jobs=None,           # å¹¶è¡Œçº¿ç¨‹æ•°ï¼ŒNone è¡¨ç¤º 1 ä¸ªï¼›-1 è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰ CPU
   
   #l1_ratio=None          # å½“ penalty='elasticnet' æ—¶ï¼Œl1_ratio ç”¨æ¥æŒ‡å®š L1/L2 æ¯”ä¾‹ï¼›å¦åˆ™ä¸º None
#)





model.fit(x_train,y_train)#åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå‚æ•°ä¼°è®¡ï¼Œè‡ªåŠ¨æ ¹æ® X_train å’Œ y_train æ±‚å‡ºæœ€ä¼˜çš„æƒé‡å‘é‡ã€‚
y_pred = model.predict(x_test)#åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹ç“œçš„å¥½å
y_proba = model.predict_proba(x_test)[:,1]#y_probaä»£è¡¨æ˜¯è¿”å›æ¦‚ç‡åˆ†å¸ƒï¼›

TP = np.sum((y_test == 1) & (y_pred == 1))
TN = np.sum((y_test == 0) & (y_pred == 0))
FP = np.sum((y_test == 0) & (y_pred == 1))
FN = np.sum((y_test == 1) & (y_pred == 0))
total = len(y_test)

accuracy_manual = (TP+TN)/total
precision_manual = TP / (TP+FP) if (TP+FP)>0 else 0
recall_manual = TP / (TP+FN) if (TP+FN)>0 else 0
specificity_manual =  TN / (TN + FP) if (TN + FP) > 0 else 0
fpr_manual = FP / (FP + TN) if (FP + TN) > 0 else 0#å‡æ­£ä¾‹ç‡
f1_manual = 2 * precision_manual * recall_manual / (precision_manual + recall_manual) if (precision_manual + recall_manual) > 0 else 0
logloss_manual = log_loss(y_test,y_proba)
 
accuracy_sklearn = accuracy_score(y_test,y_pred)
precision_sklearn = precision_score(y_test,y_pred)
recall_sklearn = recall_score(y_test,y_pred)
f1_sklearn = f1_score(y_test,y_pred)
tn,fp,fn,tp = confusion_matrix(y_test,y_pred).ravel()
specificity_sklearn = tn / (tn+fp)
fpr_sklearn = fp/(fp+tn)
logloss_sklearn = log_loss(y_test,y_proba)

print("æŒ‡æ ‡            æ‰‹åŠ¨è®¡ç®—        sklearn")
print(f"Accuracy      {accuracy_manual:.3f}         {accuracy_sklearn:.3f}")
print(f"Precision     {precision_manual:.3f}         {precision_sklearn:.3f}")
print(f"Recall        {recall_manual:.3f}         {recall_sklearn:.3f}")
print(f"Specificity   {specificity_manual:.3f}         {specificity_sklearn:.3f}")
print(f"FPR           {fpr_manual:.3f}         {fpr_sklearn:.3f}")
print(f"F1 Score      {f1_manual:.3f}         {f1_sklearn:.3f}")
print(f"Log-loss      {logloss_manual:.3f}         {logloss_sklearn:.3f}")

precisions ,recalls ,pr_thresholds = precision_recall_curve(y_test,y_proba)
ap_manual = average_precision_score(y_test,y_proba)
plt.figure(figsize = (6,4))
plt.plot(recalls , precisions ,marker = "o",label = f'AP={ap_manual: .3f}')
plt.xlabel("Recall")
plt.ylabel('Precision')
plt.title('Precision-Recall æ›²çº¿')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

fprs, tprs, roc_thresholds = roc_curve(y_test, y_proba)
auc_manual = roc_auc_score(y_test, y_proba)

plt.figure(figsize=(6, 4))
plt.plot(fprs, tprs, marker='o', label=f'AUC={auc_manual:.3f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='éšæœºçŒœæµ‹')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC æ›²çº¿')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()